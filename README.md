## Efficient attention

Linear Transformers Are Secretly Fast Weight Memory Systems, [paper](https://arxiv.org/abs/2102.11174), [code](https://github.com/ischlag/fast-weight-transformers).

Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention, [paper](https://arxiv.org/abs/2006.16236), [code](https://github.com/idiap/fast-transformers).

Linformer: Self-Attention with Linear Complexity, [paper](https://arxiv.org/abs/2006.04768), [code](https://github.com/tatp22/linformer-pytorch).

Random Feature Attention, [paper](https://arxiv.org/abs/2103.02143), [code]().

RETHINKING ATTENTION WITH PERFORMERS, [paper](https://arxiv.org/abs/2009.14794), [code](https://github.com/google-research/google-research/tree/master/performer).

Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention, [paper](https://arxiv.org/abs/2102.03902), [code](https://github.com/mlpen/Nystromformer).

Efficient Attention: Attention with Linear Complexities, [paper](https://arxiv.org/abs/1812.01243), [code](https://github.com/cmsflash/efficient-attention).
